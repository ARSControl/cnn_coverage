# -*- coding: utf-8 -*-
"""imited_gmn_coverage + rete_Prova 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_1ALL2a3rCltAc8rbY2P0a81p74mN5Q
"""

import numpy as np
from scipy.spatial import Voronoi, voronoi_plot_2d
import matplotlib.pyplot as plt
import random
from shapely import Polygon, Point, intersection
from tqdm import tqdm
from pathlib import Path
import math
import torch 

from copy import deepcopy as dc
from sklearn.mixture import GaussianMixture

ROBOTS_NUM = 6
ROBOT_RANGE = 5.0
TARGETS_NUM = 1
COMPONENTS_NUM = 1
PARTICLES_NUM = 500
AREA_W = 20.0
vmax = 1.5
SAFETY_DIST = 2.0
EPISODES = 1

path = Path().resolve()
path = (path / 'logs/rnn_coverage/')

def plot_occgrid(x, y, z, save=False, name="occgrid", ax=None):
  """
  Plot heatmap of occupancy grid.
  x, y, z : meshgrid
  """
  if save:
    path = Path("/unimore_home/mcatellani/pf-training/pics/")

  if ax is None:
    fig, ax = plt.subplots(1, 1, figsize=(6,6))
  z_min = -1.0; z_max = 1.0
  c = ax.pcolormesh(x, y, z, cmap="YlOrRd", vmin=z_min, vmax=z_max)
  ax.set_xticks([]); ax.set_yticks([])
  if save:
    save_path = path / "{}.png".format(name)
    plt.savefig(str(save_path))
  if ax is None:
    plt.show()

def mirror(points):
    mirrored_points = []

    # Define the corners of the square
    square_corners = [(-0.5*AREA_W, -0.5*AREA_W), (0.5*AREA_W, -0.5*AREA_W), (0.5*AREA_W, 0.5*AREA_W), (-0.5*AREA_W, 0.5*AREA_W)]

    # Mirror points across each edge of the square
    for edge_start, edge_end in zip(square_corners, square_corners[1:] + [square_corners[0]]):
        edge_vector = (edge_end[0] - edge_start[0], edge_end[1] - edge_start[1])

        for point in points:
            # Calculate the vector from the edge start to the point
            point_vector = (point[0] - edge_start[0], point[1] - edge_start[1])

            # Calculate the mirrored point by reflecting across the edge
            mirrored_vector = (point_vector[0] - 2 * (point_vector[0] * edge_vector[0] + point_vector[1] * edge_vector[1]) / (edge_vector[0]**2 + edge_vector[1]**2) * edge_vector[0],
                               point_vector[1] - 2 * (point_vector[0] * edge_vector[0] + point_vector[1] * edge_vector[1]) / (edge_vector[0]**2 + edge_vector[1]**2) * edge_vector[1])

            # Translate the mirrored vector back to the absolute coordinates
            mirrored_point = (edge_start[0] + mirrored_vector[0], edge_start[1] + mirrored_vector[1])

            # Add the mirrored point to the result list
            mirrored_points.append(mirrored_point)

    return mirrored_points

def gauss_pdf(x, y, mean, covariance):

  points = np.column_stack([x.flatten(), y.flatten()])
  # Calculate the multivariate Gaussian probability
  exponent = -0.5 * np.sum((points - mean) @ np.linalg.inv(covariance) * (points - mean), axis=1)
  coefficient = 1 / np.sqrt((2 * np.pi) ** 2 * np.linalg.det(covariance))
  prob = coefficient * np.exp(exponent)

  return prob

def gmm_pdf(x, y, means, covariances, weights):
  prob = 0.0
  s = len(means)
  for i in range(s):
    prob += weights[i] * gauss_pdf(x, y, means[i], covariances[i])

  return prob

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

path = Path("/home/mattia/coverage_cnn/datasets/coverage")
files = [x for x in path.glob("**/*") if x.is_file()]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ", device)

imgs = []
vels = []
FILES_NUM = len(files)//2

for i in range(FILES_NUM):
  imgs.append(np.load(str(path / f"test{i}.npy")))
  vels.append(np.load(str(path / f"vels{i}.npy")))

import torch
import numpy as np
#c=0

#a=torch.tensor(imgs[1])
#b=torch.tensor(imgs[2])

#c= torch.cat((a,b), dim=0)
#c.shape

c=torch.empty((0,6,64,64))
for i in range(200):
  a=torch.tensor(imgs[i], dtype=torch.float32)
  #a=imgs[i]
  c=torch.cat((c,a), dim=0)


r1= c[:, 0, :, :]
r2= c[:, 1, :, :]
r3= c[:, 2, :, :]
r4= c[:, 3, :, :]
r5= c[:, 4, :, :]
r6= c[:, 5, :, :]

p1 = torch.cat((r1, r2, r3, r4, r5, r6))


f=torch.empty((0,6,2))
for j in range(200):
  e=torch.tensor(vels[j], dtype=torch.float32)
  #a=imgs[i]
  f=torch.cat((f,e), dim=0)

v1= f[:, 0, :]
v2= f[:, 1, :]
v3= f[:, 2, :]
v4= f[:, 3, :]
v5= f[:, 4, :]
v6= f[:, 5, :]

w1 = torch.cat((v1, v2, v3, v4, v5, v6))

train_size = int(p1.shape[0]*0.75)
print("Training size: ", train_size)

X_train, Y_train, X_test, Y_test = p1[:train_size], w1[:train_size], p1[train_size:], w1[train_size:]
X_train = X_train.unsqueeze(1)
X_test = X_test.unsqueeze(1)
Y_train = Y_train.unsqueeze(1)
Y_test = Y_test.unsqueeze(1)
X_train, X_test, Y_train, Y_test = X_train.to(device), X_test.to(device), Y_train.to(device), Y_test.to(device)
print(f"Train/Test shapes: {X_train.shape}, {Y_train.shape}, {X_test.shape}, {Y_test.shape}")

# p1.shape, w1.shape, X_train.shape, Y_train.shape, X_test.shape, Y_test.shape

from torch.utils.data import TensorDataset, DataLoader

num_classes = 2
num_epochs = 20
batch_size = 32
learning_rate = 0.001

input_size = 64
sequence_length = 64
hidden_size = 32
num_layers = 3

train_dataset = TensorDataset(X_train, Y_train)
test_dataset  = TensorDataset(X_test,  Y_test)

train_loader = DataLoader(train_dataset, shuffle=False, batch_size=32)
test_loader  = DataLoader(test_dataset,  shuffle=False, batch_size=32)

for _, batch in enumerate(train_loader):
    x_batch, y_batch = batch[0], batch[1]
    print(x_batch.shape, y_batch.shape)
    break

from torch import nn
from torch.nn import functional as F

# Hyper-parameters
# input_size = 784 # 28x28


class CNN_LSTM(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, num_classes):
    super(CNN_LSTM, self).__init__()
    self.cnn = nn.Sequential(
        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )
    self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward(self, x):
    #cnn takes input of shape (batch_size, channels, seq_len)
    # x = x.permute(0, 2, 1)
    out = self.cnn(x)
    # lstm takes input of shape (batch_size, seq_len, input_size)
    # out = out.permute(0, 2, 1)
    # print(f"shape after cnn: {out.shape}")
    out = out.view(out.shape[0], -1, out.shape[1])
    # print("Reshaped output: ", out.shape)
    out, _ = self.lstm(out)
    out = self.fc(out[:, -1, :])
    return out

cnn_lstm = CNN_LSTM(input_size, hidden_size, num_layers, num_classes).to(device)
from torch import optim
# Loss and optimizer
#criterion = nn.CrossEntropyLoss()
criterion=nn.MSELoss()

optimizer = torch.optim.Adam(cnn_lstm.parameters(), lr=learning_rate)


cnn_lstm.train()

# Train the model
total_step = len(train_loader)
loss_values=[]
for epoch in range(num_epochs):
  for i, (images, labels) in enumerate(train_loader):
    # clear gradients for this training step
    optimizer.zero_grad()

    output = cnn_lstm(images)
    #labels=labels.squeeze(1)
    loss = criterion(output.unsqueeze(1), labels)

    # backpropagation, compute gradients
    loss.backward()
    # apply gradients
    optimizer.step()

    running_loss =+ loss.item()

        
  print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
  #if (i+1) % 619 == 0:
    # print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')
    # print(f"Predicted: {output[epoch+1]}")
    #print(f"Lables: {labels[epoch+1]}")


  loss_values.append(running_loss)

PATH = path/'safety_model.pt'
# torch.save(cnn_lstm.state_dict(), str(PATH))
print("Model saved")


'''

model = torch.load('salvataggio.pt')

print(model)

for episode in range(EPISODES):
  targets = np.zeros((TARGETS_NUM, 1, 2))
  for i in range(TARGETS_NUM):
    targets[i, 0, 0] = -0.5*(AREA_W-1) + (AREA_W-1) * np.random.rand(1,1)
    targets[i, 0, 1] = -0.5*(AREA_W-1) + (AREA_W-1) * np.random.rand(1,1)

  # plt.plot([-0.5*AREA_W, 0.5*AREA_W], [-0.5*AREA_W, -0.5*AREA_W], c='tab:blue', label="Environment")
  # plt.plot([0.5*AREA_W, 0.5*AREA_W], [-0.5*AREA_W, 0.5*AREA_W], c='tab:blue')
  # plt.plot([0.5*AREA_W, -0.5*AREA_W], [0.5*AREA_W, 0.5*AREA_W], c='tab:blue')
  # plt.plot([-0.5*AREA_W, -0.5*AREA_W], [0.5*AREA_W, -0.5*AREA_W], c='tab:blue')
  # plt.scatter(targets[:, :, 0], targets[:, :, 1], c='tab:orange', label="Targets")
  # # plt.legend()
  # plt.show()

  STD_DEV = 2.0
  samples = np.zeros((TARGETS_NUM, PARTICLES_NUM, 2))
  for k in range(TARGETS_NUM):
    for i in range(PARTICLES_NUM):
      samples[k, i, :] = targets[k, 0, :] + STD_DEV * np.random.randn(1, 2)

  # Fit GMM
  samples = samples.reshape((TARGETS_NUM*PARTICLES_NUM, 2))
  print(samples.shape)
  gmm = GaussianMixture(n_components=COMPONENTS_NUM, covariance_type='full', max_iter=1000)
  gmm.fit(samples)

  means = gmm.means_
  covariances = gmm.covariances_
  mix = gmm.weights_

  print(f"Means: {means}")
  print(f"Covs: {covariances}")
  print(f"Mix: {mix}")


  ## -------- Generate decentralized probability grid ---------
  GRID_STEPS = 64
  s = AREA_W/GRID_STEPS     # step

  xg = np.linspace(-0.5*AREA_W, 0.5*AREA_W, GRID_STEPS)
  yg = np.linspace(-0.5*AREA_W, 0.5*AREA_W, GRID_STEPS)
  Xg, Yg = np.meshgrid(xg, yg)
  Xg.shape
  print(Xg.shape)

  Z = gmm_pdf(Xg, Yg, means, covariances, mix)
  Z = Z.reshape(GRID_STEPS, GRID_STEPS)
  Zmax = np.max(Z)
  Z = Z / Zmax



  # ---------- Simulate episode ---------
  # ROBOTS_NUM = np.random.randint(6, ROBOTS_MAX)
  ROBOTS_NUM = 6
  converged = False
  NUM_STEPS = 50
  points = -0.5*AREA_W + AREA_W * np.random.rand(ROBOTS_NUM, 2)
  robots_hist = np.zeros((1, points.shape[0], points.shape[1]))
  robots_hist[0, :, :] = points
  vis_regions = []
  discretize_precision = 0.5

  imgs = np.zeros((1, ROBOTS_NUM, GRID_STEPS, GRID_STEPS))
  vels = np.zeros((1, ROBOTS_NUM, 2))

  r_step = 2 * ROBOT_RANGE / GRID_STEPS
  for s in range(1, NUM_STEPS+1):
    row = 0
    if s > 5:
      row = 1

    # mirror points across each edge of the env
    dummy_points = np.zeros((5*ROBOTS_NUM, 2))
    dummy_points[:ROBOTS_NUM, :] = points
    mirrored_points = mirror(points)
    mir_pts = np.array(mirrored_points)
    dummy_points[ROBOTS_NUM:, :] = mir_pts

    # Voronoi partitioning
    vor = Voronoi(dummy_points)

    conv = True
    lim_regions = []
    img_s = np.zeros((ROBOTS_NUM, GRID_STEPS, GRID_STEPS))
    vel_s = np.zeros((ROBOTS_NUM, 2))
    for idx in range(ROBOTS_NUM):
      # Save grid
      p_i = vor.points[idx]
      xg_i = np.linspace(-ROBOT_RANGE, ROBOT_RANGE, GRID_STEPS)
      yg_i = np.linspace(-ROBOT_RANGE, ROBOT_RANGE, GRID_STEPS)
      Xg_i, Yg_i = np.meshgrid(xg_i, yg_i)
      Z_i = gmm_pdf(Xg_i, Yg_i, means-p_i, covariances, mix)
      Z_i = Z_i.reshape(GRID_STEPS, GRID_STEPS)
      Zmax_i = np.max(Z_i)
      Z_i = Z_i / Zmax_i

      neighs = np.delete(vor.points[:ROBOTS_NUM], idx, 0)
      local_pts = neighs - p_i

      # Remove undetected neighbors
      undetected = []
      for i in range(local_pts.shape[0]):
        if local_pts[i, 0] < -ROBOT_RANGE or local_pts[i, 0] > ROBOT_RANGE or local_pts[i, 1] < -ROBOT_RANGE or local_pts[i, 1] > ROBOT_RANGE:
          undetected.append(i)

      local_pts = np.delete(local_pts, undetected, 0)

      img_i = dc(Z_i)
      for i in range(GRID_STEPS):
        for j in range(GRID_STEPS):
          # jj = GRID_STEPS-1-j
          p_ij = np.array([-ROBOT_RANGE+j*r_step, -ROBOT_RANGE+i*r_step])
          # print(f"Point ({i},{j}): {p_ij}")
          for n in local_pts:
            if np.linalg.norm(n - p_ij) <= SAFETY_DIST:
              img_i[i, j] = -1.0

          # Check if outside boundaries
          p_w = p_ij + p_i
          if p_w[0] < -0.5*AREA_W or p_w[0] > 0.5*AREA_W or p_w[1] < -0.5*AREA_W or p_w[1] > 0.5*AREA_W:
            img_i[i, j] = -1.0

      img_s[idx, :, :] = img_i
      #print(img_s.shape)
    #print(img_s.shape)

    imgs = np.concatenate((imgs, np.expand_dims(img_s, 0)))
    print(imgs.shape)
    #imgs.reshape(-1,64,64)
    #print(imgs.shape)
    #vels = np.concatenate((vels, np.expand_dims(vel_s, 0)))
      #points[idx, :]= robot+vel

    #print(imgs.shape)
  imgs=torch.tensor(imgs, dtype=torch.float)
  r1= imgs[:, 0, :, :]
  r2= imgs[:, 1, :, :]
  r3= imgs[:, 2, :, :]
  r4= imgs[:, 3, :, :]
  r5= imgs[:, 4, :, :]
  r6= imgs[:, 5, :, :]
  r1.shape

  p1 = torch.cat((r1, r2, r3, r4, r5, r6))
  print(p1.shape)




  cnn_lstm.eval()
  #for epoch in range(2):
  with torch.no_grad():
        #print(img_s.shape)

    vels=cnn_lstm(p1)
    print(vels.shape)
    print(vels)
        #print(vel_s)
      #plt.plot(vel_pred[0])


      #points[idx, :] = robot + vel
      #if dist > 0.1:

      #if error>0.1:
       # conv = False


    #imgs = np.concatenate((imgs, np.expand_dims(img_s, 0)))
    #vels = np.concatenate((vels, np.expand_dims(vel_s, 0)))

    # Save positions for visualization
   # if s == 1:
    #  vis_regions.append(lim_regions)
    #robots_hist = np.vstack((robots_hist, np.expand_dims(points, axis=0)))
   # vis_regions.append(lim_regions)

    #if conv:
    #  print(f"Converged in {s} iterations")
    #  break
    # axs[row, s-1-5*row].scatter(points[:, 0], points[:, 1])

  imgs = imgs[1:]
  vels = vels[1:]
  imgs.shape

  '''